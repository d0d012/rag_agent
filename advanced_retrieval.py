from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore 
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import os
from dotenv import load_dotenv
load_dotenv() #loading env file and getting api key variable

api_key = os.getenv("OPENAI_API_KEY")

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

qdrant = QdrantVectorStore.from_existing_collection(
    embedding=embeddings,
    url="http://localhost:6333",
    collection_name="test_collection"
)

llm = ChatOpenAI(
    model="gpt-4o-mini",
)

messages = [SystemMessage (content="Sen bir yapay zeka asistan覺s覺n. Kullan覺c覺n覺n sorusunu vekt繹r veritaban覺nda aramak i癟in 3 farkl覺 alternatif arama sorgusu (query) 羹ret. Sadece sorgular覺 alt alta yaz."),
            HumanMessage(content="Go dilindeki aray羹zler, dier dillerdeki regresyon testlerini nas覺l etkiler?")]

response = llm.invoke(messages)
lines = response.content.splitlines()
lines.append("Go dilindeki aray羹zler, dier dillerdeki regresyon testlerini nas覺l etkiler?")

all_results = []

for query in lines:
    all_results.append(qdrant.similarity_search(k=3, query=query)) #list of lists (list of related documents to each query)

rrf_scores = {}

for result in all_results:
    for i, document in enumerate(result):
        score = float(1 / (i+61))
        if document.page_content in rrf_scores:
            rrf_scores[document.page_content] += score
        else:
            rrf_scores.setdefault(document.page_content,score)

sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

print("\n--- Best Results ---")
for j in range(min(3, len(sorted_docs))):
    metin = sorted_docs[j][0]
    score = sorted_docs[j][1]
    print(f"\n[Rank {j+1}] RRF Score: {score:.4f}")
    print(f"Text: {metin[:200]}...") 

# --- ST KISIM AYNI ---

# Soruyu bir deikene at覺yoruz ki aa覺da da kullanabilelim
original_query = "Go dilindeki aray羹zler, dier dillerdeki regresyon testlerini nas覺l etkiler?"

messages = [
    SystemMessage(content="Sen bir yapay zeka asistan覺s覺n. Kullan覺c覺n覺n sorusunu vekt繹r veritaban覺nda aramak i癟in 3 farkl覺 alternatif arama sorgusu (query) 羹ret. Sadece sorgular覺 alt alta yaz."),
    HumanMessage(content=original_query) # Deikeni burada kulland覺k
]

response = llm.invoke(messages)
lines = response.content.splitlines()
lines.append(original_query) # Orijinal soruyu da varyasyonlara ekledik

all_results = []

for query in lines:
    all_results.append(qdrant.similarity_search(k=3, query=query))

rrf_scores = {}

for result in all_results:
    for i, document in enumerate(result):
        score = float(1 / (i+61))
        if document.page_content in rrf_scores:
            rrf_scores[document.page_content] += score
        else:
            rrf_scores.setdefault(document.page_content, score)

sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

print("\n--- Best Results ---")
for j in range(min(3, len(sorted_docs))):
    metin = sorted_docs[j][0]
    score = sorted_docs[j][1]
    print(f"\n[Rank {j+1}] RRF Score: {score:.4f}")
    print(f"Text: {metin[:200]}...") 

print("\n\n--- GENERATION ---")

context_text = "\n\n---\n\n".join([doc[0] for doc in sorted_docs[:2]])

generation_messages = [ #prompt scheme
    SystemMessage(content=(
        "Sen uzman bir yaz覺l覺m mimar覺s覺n. "
        "Kullan覺c覺n覺n sorusunu SADECE sana verilen 'Balam' metnini kullanarak cevapla. "
        "Eer sorunun cevab覺 balamda a癟覺k癟a belirtilmiyorsa veya iki kavram aras覺nda balamda bir iliki kurulmam覺sa, "
        "uydurma ve d羹r羹st癟e 'Verilen dok羹manlarda bu konuyla ilgili bir balant覺/cevap bulunmamaktad覺r.' de."
    )),
    HumanMessage(content=f"Balam:\n{context_text}\n\nKullan覺c覺n覺n Sorusu: {original_query}")
]

#llm invoke
print("LLM salanan balam覺 okuyor ve cevab覺 羹retiyor...\n")
final_response = llm.invoke(generation_messages)

print(" AI YANITI:")
print(final_response.content)